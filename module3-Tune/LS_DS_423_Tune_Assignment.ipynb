{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LS_DS_433_Tune_Assignment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "nteract": {
      "version": "0.22.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGGrt9EYlCqY"
      },
      "source": [
        "\n",
        "\n",
        "# Tune Practice\n",
        "\n",
        "## *Data Science Unit 4 Sprint 2 Assignment 3*\n",
        "\n",
        "# Gridsearch Hyperparameters\n",
        "\n",
        "In the guided project, you learned how to use sklearn's GridsearchCV and keras-tuner library to tune the hyperparamters of a neural network model. For your module project you'll continue using these two libraries however we are going to make things a little more interesting for you. \n",
        "\n",
        "Continue to use TensorFlow Keras & a sample of the [Quickdraw dataset](https://github.com/googlecreativelab/quickdraw-dataset) to build a sketch classification model. The dataset has been sampled to only 10 classes and 10000 observations per class. \n",
        "\n",
        "\n",
        "\n",
        "**Don't forgot to switch to GPU on Colab!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2n8za--oicAq",
        "outputId": "4b22f1e4-94f5-4848-9749-920a3385e918",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install keras-tuner"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras-tuner\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/ec/1ef246787174b1e2bb591c95f29d3c1310070cad877824f907faba3dade9/keras-tuner-1.0.2.tar.gz (62kB)\n",
            "\r\u001b[K     |█████▏                          | 10kB 14.3MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 20kB 19.6MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 30kB 21.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 40kB 24.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 51kB 26.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 61kB 28.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 7.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (20.9)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (1.19.5)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (0.8.9)\n",
            "Collecting terminaltables\n",
            "  Downloading https://files.pythonhosted.org/packages/9b/c4/4a21174f32f8a7e1104798c445dacdc1d4df86f2f26722767034e4de4bff/terminaltables-3.1.0.tar.gz\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (2.23.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (0.22.2.post1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->keras-tuner) (2.4.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (3.0.4)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->keras-tuner) (1.0.1)\n",
            "Building wheels for collected packages: keras-tuner, terminaltables\n",
            "  Building wheel for keras-tuner (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-tuner: filename=keras_tuner-1.0.2-cp37-none-any.whl size=78938 sha256=10842bf2234601ef26d20aa253bd88adb2756c511ecd8e99ef1df53390cdaff8\n",
            "  Stored in directory: /root/.cache/pip/wheels/bb/a1/8a/7c3de0efb3707a1701b36ebbfdbc4e67aedf6d4943a1f463d6\n",
            "  Building wheel for terminaltables (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for terminaltables: filename=terminaltables-3.1.0-cp37-none-any.whl size=15356 sha256=8f35d0bd967ad74cd8c59b0ca60e8ab96425c333b31c86955b7a3f1cd0703cb2\n",
            "  Stored in directory: /root/.cache/pip/wheels/30/6b/50/6c75775b681fb36cdfac7f19799888ef9d8813aff9e379663e\n",
            "Successfully built keras-tuner terminaltables\n",
            "Installing collected packages: terminaltables, colorama, keras-tuner\n",
            "Successfully installed colorama-0.4.4 keras-tuner-1.0.2 terminaltables-3.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJis-lTTh_n5"
      },
      "source": [
        "# native python libraries imports \n",
        "import math\n",
        "from time import time\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# sklearn imports \n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# keras imports \n",
        "import keras\n",
        "from keras import Sequential\n",
        "from keras.layers import Dense\n",
        "from kerastuner.tuners import RandomSearch, BayesianOptimization, Sklearn\n",
        "from kerastuner.engine.hyperparameters import HyperParameters\n",
        "from keras.activations import relu, sigmoid\n",
        "from keras.optimizers import Adam, SGD\n",
        "from keras.utils import get_file\n",
        "\n",
        "# required for compatibility between sklearn and keras\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dy6XyFmKh_n6"
      },
      "source": [
        "def load_quickdraw10():\n",
        "    \"\"\"\n",
        "    Fill out this doc string, and comment the code, for practice in writing the kind of code that will get you hired. \n",
        "    \"\"\"\n",
        "    \n",
        "    URL_ = \"https://github.com/LambdaSchool/DS-Unit-4-Sprint-2-Neural-Networks/blob/main/quickdraw10.npz?raw=true\"\n",
        "    \n",
        "    path_to_zip = get_file('./quickdraw10.npz', origin=URL_, extract=False)\n",
        "\n",
        "    data = np.load(path_to_zip)\n",
        "    \n",
        "    # normalize your image data\n",
        "    max_pixel_value = 255\n",
        "    X = data['arr_0']/max_pixel_value\n",
        "    Y = data['arr_1']\n",
        "        \n",
        "    return train_test_split(X, Y, shuffle=True)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGMY3pJ7h_n7",
        "outputId": "364b08a8-74d3-4823-dad9-438558132318",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "X_train, X_test, y_train, y_test = load_quickdraw10()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/LambdaSchool/DS-Unit-4-Sprint-2-Neural-Networks/blob/main/quickdraw10.npz?raw=true\n",
            "25427968/25421363 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlxLKnP0h_n7",
        "outputId": "a1698b38-1d8f-458d-edfe-5ca0c4ca95b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(75000, 784)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gKIgypUh_n7",
        "outputId": "d4baee2c-d2ad-4239-b2ca-6c9d37979bed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "y_train.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(75000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FrLlgvxh_n8"
      },
      "source": [
        "_____\n",
        "\n",
        "# Experiment 1\n",
        "\n",
        "## Tune Hyperperameters using Enhanced GridsearchCV \n",
        "\n",
        "We are going to use GridsearchCV again to tune a deep learning model however we are going to add some additional functionality to our gridsearch. Specifically, we are going to automate away the generation of how many nodes to use in a layer and how many layers to use in a model! \n",
        "\n",
        "By the way, yes, there is a function within a function. Try to not let that bother you. An alternative to this would be to create a class. If you're up for the challenge give it a shot. However, consider this a stretch goal that you come back to after you finish going through this assignment. \n",
        "\n",
        "\n",
        "### Objective \n",
        "\n",
        "The objective of this experiment is to show you how to automate the generation of layers and layer nodes for the purposes of gridsearch. Up until now, we've been manually selecting the number of layers and layer nodes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USXjs7Hk71Hy"
      },
      "source": [
        "# Function to create model, required for KerasClassifier\n",
        "def create_model(n_layers,  first_layer_nodes, last_layer_nodes, act_funct =\"relu\", negative_node_incrementation=True):\n",
        "    \"\"\"\"\n",
        "    Returns a complied keras model \n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    n_layers: int \n",
        "        number of hidden layers in model \n",
        "        To be clear, this excludes the input and output layer.\n",
        "        \n",
        "    first_layer_nodes: int\n",
        "        Number of nodes in the first hidden layer \n",
        "\n",
        "    last_layer_nodes: int\n",
        "        Number of nodes in the last hidden layer (this is the layer just prior to the output layer)\n",
        "        \n",
        "     act_funct: string \n",
        "         Name of activation function to use in hidden layers (this excludes the output layler)\n",
        "        \n",
        "    Returns\n",
        "    -------\n",
        "    model: keras object \n",
        "    \"\"\"\n",
        "    \n",
        "    def gen_layer_nodes(n_layers, first_layer_nodes, last_layer_nodes, negative_node_incrementation=True):\n",
        "        \"\"\"\n",
        "        Generates and returns the number of nodes in each hidden layer. \n",
        "        To be clear, this excludes the input and output layer. \n",
        "\n",
        "        Note\n",
        "        ----\n",
        "        Number of nodes in each layer is linearly incremented. \n",
        "        For example, gen_layer_nodes(5, 500, 100) will generate [500, 400, 300, 200, 100]\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_layers: int\n",
        "            Number of hidden layers\n",
        "            This values should be 2 or greater \n",
        "\n",
        "        first_layer_nodes: int\n",
        "\n",
        "        last_layer_nodes: int\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        layers: list of ints\n",
        "            Contains number of nodes for each layer \n",
        "        \"\"\"\n",
        "\n",
        "        # throws an error if n_layers is less than 2 \n",
        "        assert n_layers >= 2, \"n_layers must be 2 or greater\"\n",
        "\n",
        "        layers = []\n",
        "\n",
        "        # PROTIP: IF YOU WANT THE NODE INCREMENTATION TO BE SPACED DIFFERENTLY\n",
        "        # THEN YOU'LL NEED TO CHANGE THE WAY THAT IT'S CALCULATED - HAVE FUN!\n",
        "        # when set to True number of nodes are decreased for subsequent layers \n",
        "        if negative_node_incrementation:\n",
        "            # subtract this amount from previous layer's nodes in order to increment towards smaller numbers \n",
        "            nodes_increment = (last_layer_nodes - first_layer_nodes)/ (n_layers-1)\n",
        "            \n",
        "        # when set to False number of nodes are increased for subsequent layers\n",
        "        else:\n",
        "            # add this amount from previous layer's nodes in order to increment towards larger numbers \n",
        "            nodes_increment = (first_layer_nodes - last_layer_nodes)/ (n_layers-1)\n",
        "\n",
        "        nodes = first_layer_nodes\n",
        "\n",
        "        for i in range(1, n_layers+1):\n",
        "\n",
        "            layers.append(math.ceil(nodes))\n",
        "\n",
        "            # increment nodes for next layer \n",
        "            nodes = nodes + nodes_increment\n",
        "\n",
        "        return layers\n",
        "    \n",
        "    # create model\n",
        "    model = Sequential()\n",
        "    \n",
        "    n_nodes = gen_layer_nodes(n_layers, first_layer_nodes, last_layer_nodes, negative_node_incrementation)\n",
        "    \n",
        "    for i in range(1, n_layers):\n",
        "        if i==1:\n",
        "            model.add(Dense(first_layer_nodes, input_dim=X_train.shape[1], activation=act_funct))\n",
        "        else:\n",
        "            model.add(Dense(n_nodes[i-1], activation=act_funct))\n",
        "            \n",
        "            \n",
        "    # output layer \n",
        "    model.add(Dense(10, # 10 unit/neurons in output layer because we have 10 possible labels to predict  \n",
        "                    activation='softmax')) # use softmax for a label set greater than 2            \n",
        "    \n",
        "    # Compile model\n",
        "    model.compile(loss='sparse_categorical_crossentropy', \n",
        "                  optimizer='adam', # adam is a good default optimizer \n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    # do not include model.fit() inside the create_model function\n",
        "    # KerasClassifier is expecting a complied model \n",
        "    return model\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXS0PRK5h_n9"
      },
      "source": [
        "## Explore create_model\n",
        "\n",
        "Let's build a few different models in order to understand how the above code works in practice. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIcn8Nhxh_n-"
      },
      "source": [
        "### Build model \n",
        "\n",
        "Use `create_model` to build a model. \n",
        "\n",
        "- Set `n_layers = 10` \n",
        "- Set `first_layer_nodes = 500`\n",
        "- Set `last_layer_nodes = 100`\n",
        "- Set `act_funct = \"relu\"`\n",
        "- Make sure that `negative_node_incrementation = True`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "5dcf5c585f07629a03086cf57ba53615",
          "grade": false,
          "grade_id": "cell-86d63e89a21223de",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "_--OiOTuh_n-",
        "outputId": "59a9ef15-a2c2-46bc-9aac-bb8ee843cb02",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# use create_model to create a model \n",
        "\n",
        "# YOUR CODE HERE\n",
        "model = create_model(n_layers=10, first_layer_nodes=500, last_layer_nodes=100, act_funct='relu', negative_node_incrementation=True)\n",
        "model.summary()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 500)               392500    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 456)               228456    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 412)               188284    \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 367)               151571    \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 323)               118864    \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 278)               90072     \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 234)               65286     \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 189)               44415     \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 145)               27550     \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 10)                1460      \n",
            "=================================================================\n",
            "Total params: 1,308,458\n",
            "Trainable params: 1,308,458\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDLHfgkKh_n-",
        "outputId": "058eed2b-acd7-4733-9ff5-2c13e645374d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# run model.summary() and make sure that you understand the model architecture that you just built \n",
        "# Notice in the model summary how the number of nodes have been linearly incremented in decreasing values. \n",
        "model.summary()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 500)               392500    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 456)               228456    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 412)               188284    \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 367)               151571    \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 323)               118864    \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 278)               90072     \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 234)               65286     \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 189)               44415     \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 145)               27550     \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 10)                1460      \n",
            "=================================================================\n",
            "Total params: 1,308,458\n",
            "Trainable params: 1,308,458\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7Y1K0wgh_n-"
      },
      "source": [
        "### Build model \n",
        "\n",
        "Use `create_model` to build a model. \n",
        "\n",
        "- Set `n_layers = 10` \n",
        "- Set `first_layer_nodes = 500`\n",
        "- Set `last_layer_nodes = 100`\n",
        "- Set `act_funct = \"relu\"`\n",
        "- Make sure that `negative_node_incrementation = False`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "e0722533c325d699f4842e874e43720e",
          "grade": false,
          "grade_id": "cell-99d563a291231a7b",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "1O4pyKxLh_n_"
      },
      "source": [
        "# use create_model to create a model \n",
        "\n",
        "# YOUR CODE HERE\n",
        "model = create_model(n_layers=10, first_layer_nodes=500, last_layer_nodes=100, act_funct='relu', negative_node_incrementation=False)\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBsNaKk4h_n_",
        "outputId": "d2728a34-399c-4213-e90a-6d5f5c3a6991",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# run model.summary() and make sure that you understand the model architecture that you just built \n",
        "# Notice in the model summary how the number of nodes have been linearly incremented in increasing values.\n",
        "# The output layer must have 10 nodes because there are 10 labels to predict \n",
        "model.summary()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_10 (Dense)             (None, 500)               392500    \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 545)               273045    \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 589)               321594    \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 634)               374060    \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 678)               430530    \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 723)               490917    \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 767)               555308    \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 812)               623616    \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 856)               695928    \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 10)                8570      \n",
            "=================================================================\n",
            "Total params: 4,166,068\n",
            "Trainable params: 4,166,068\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlhUlODih_n_",
        "outputId": "27e5878c-9ab3-42a5-aabc-8d03a0a3acb6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# feel free to play around with parameters to gain additional insight as to how the create_model function works \n",
        "\n",
        "model = create_model(n_layers=10, first_layer_nodes=500, last_layer_nodes=100, act_funct='gelu', negative_node_incrementation=True)\n",
        "model.summary()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_20 (Dense)             (None, 500)               392500    \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 456)               228456    \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 412)               188284    \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 367)               151571    \n",
            "_________________________________________________________________\n",
            "dense_24 (Dense)             (None, 323)               118864    \n",
            "_________________________________________________________________\n",
            "dense_25 (Dense)             (None, 278)               90072     \n",
            "_________________________________________________________________\n",
            "dense_26 (Dense)             (None, 234)               65286     \n",
            "_________________________________________________________________\n",
            "dense_27 (Dense)             (None, 189)               44415     \n",
            "_________________________________________________________________\n",
            "dense_28 (Dense)             (None, 145)               27550     \n",
            "_________________________________________________________________\n",
            "dense_29 (Dense)             (None, 10)                1460      \n",
            "=================================================================\n",
            "Total params: 1,308,458\n",
            "Trainable params: 1,308,458\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxaLM7zwh_n_"
      },
      "source": [
        "Ok, now that we've played around a bit with  `create_model` in order to understand how it works, let's build a much simpler model that we'll be running gridsearches. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHuggC6Wh_n_"
      },
      "source": [
        "### Build model \n",
        "\n",
        "Use `create_model` to build a model. \n",
        "\n",
        "- Set `n_layers = 2` \n",
        "- Set `first_layer_nodes = 500`\n",
        "- Set `last_layer_nodes = 100`\n",
        "- Set `act_funct = \"relu\"`\n",
        "- Make sure that `negative_node_incrementation = True`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "606b85d0ba4531836f97caf6850297f8",
          "grade": false,
          "grade_id": "cell-4ca6c5e51302fd10",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "OdrTyhvTh_oA"
      },
      "source": [
        "# use create_model to create a model \n",
        "\n",
        "# YOUR CODE HERE\n",
        "model = create_model(n_layers=2, first_layer_nodes=500, last_layer_nodes=100, act_funct='gelu', negative_node_incrementation=True)\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_w3o-rRh_oA",
        "outputId": "271935fe-ff7b-4f43-bd23-26c6d75514a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# run model.summary() and make sure that you understand the model architecture that you just built \n",
        "model.summary()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_30 (Dense)             (None, 500)               392500    \n",
            "_________________________________________________________________\n",
            "dense_31 (Dense)             (None, 10)                5010      \n",
            "=================================================================\n",
            "Total params: 397,510\n",
            "Trainable params: 397,510\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WVZfh2lh_oA"
      },
      "source": [
        "# define the grid search parameters\n",
        "param_grid = {'n_layers': [2, 3],\n",
        "              'epochs': [3], \n",
        "              \"first_layer_nodes\": [500, 300],\n",
        "              \"last_layer_nodes\": [100, 50]\n",
        "             }"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P44zl15Eh_oB"
      },
      "source": [
        "model = KerasClassifier(create_model)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_O5JF33h_oB",
        "outputId": "2b205741-11d5-4f47-fda6-14a629e05552",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Create Grid Search\n",
        "grid = GridSearchCV(estimator=model, \n",
        "                    param_grid=param_grid, \n",
        "                    n_jobs=-2, \n",
        "                    verbose=1, \n",
        "                    cv=3)\n",
        "\n",
        "grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "# Report Results\n",
        "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
        "\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-2)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.8342 - accuracy: 0.7447\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4527 - accuracy: 0.8643\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.3591 - accuracy: 0.8933\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4682 - accuracy: 0.8649\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.8253 - accuracy: 0.7479\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4558 - accuracy: 0.8642\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.3579 - accuracy: 0.8900\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4564 - accuracy: 0.8682\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.8279 - accuracy: 0.7466\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4567 - accuracy: 0.8649\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.3555 - accuracy: 0.8923\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4735 - accuracy: 0.8615\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.7949 - accuracy: 0.7526\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.4291 - accuracy: 0.8686\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.3378 - accuracy: 0.8958\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.4488 - accuracy: 0.8675\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.7909 - accuracy: 0.7531\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.4374 - accuracy: 0.8676\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.3403 - accuracy: 0.8950\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.4623 - accuracy: 0.8643\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.7960 - accuracy: 0.7517\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.4337 - accuracy: 0.8689\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.3372 - accuracy: 0.8947\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4620 - accuracy: 0.8632\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.8346 - accuracy: 0.7449\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4564 - accuracy: 0.8627\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.3539 - accuracy: 0.8949\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4660 - accuracy: 0.8677\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.8301 - accuracy: 0.7449\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4581 - accuracy: 0.8638\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.3574 - accuracy: 0.8923\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4685 - accuracy: 0.8654\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.8280 - accuracy: 0.7480\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4585 - accuracy: 0.8666\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.3550 - accuracy: 0.8932\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4743 - accuracy: 0.8636\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.7961 - accuracy: 0.7510\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.4344 - accuracy: 0.8673\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.3425 - accuracy: 0.8955\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4619 - accuracy: 0.8617\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.8121 - accuracy: 0.7453\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.4252 - accuracy: 0.8699\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.3363 - accuracy: 0.8979\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4632 - accuracy: 0.8673\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.7920 - accuracy: 0.7540\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.4295 - accuracy: 0.8688\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.3307 - accuracy: 0.8968\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.4502 - accuracy: 0.8679\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8696 - accuracy: 0.7356\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.4824 - accuracy: 0.8569\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.3920 - accuracy: 0.8815\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.4697 - accuracy: 0.8632\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8796 - accuracy: 0.7293\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.4854 - accuracy: 0.8573\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.3968 - accuracy: 0.8807\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.4742 - accuracy: 0.8614\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8532 - accuracy: 0.7388\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.4759 - accuracy: 0.8606\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.3790 - accuracy: 0.8884\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.4712 - accuracy: 0.8617\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.8326 - accuracy: 0.7412\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4541 - accuracy: 0.8627\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.3502 - accuracy: 0.8913\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.4693 - accuracy: 0.8602\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.8316 - accuracy: 0.7431\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4492 - accuracy: 0.8628\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.3518 - accuracy: 0.8919\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.4650 - accuracy: 0.8633\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.8339 - accuracy: 0.7380\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4517 - accuracy: 0.8646\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.3561 - accuracy: 0.8897\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.4797 - accuracy: 0.8589\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8677 - accuracy: 0.7369\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.4712 - accuracy: 0.8591\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.3865 - accuracy: 0.8846\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.4751 - accuracy: 0.8622\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8637 - accuracy: 0.7351\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.4778 - accuracy: 0.8588\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.3821 - accuracy: 0.8872\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.4694 - accuracy: 0.8613\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8539 - accuracy: 0.7415\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.4764 - accuracy: 0.8594\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.3852 - accuracy: 0.8857\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.4752 - accuracy: 0.8604\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.8213 - accuracy: 0.7433\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4510 - accuracy: 0.8616\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.3580 - accuracy: 0.8926\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.4746 - accuracy: 0.8623\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.8413 - accuracy: 0.7364\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4515 - accuracy: 0.8634\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.3656 - accuracy: 0.8868\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.4668 - accuracy: 0.8635\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.8304 - accuracy: 0.7438\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4555 - accuracy: 0.8612\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.3550 - accuracy: 0.8898\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.4766 - accuracy: 0.8583\n",
            "Epoch 1/3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-2)]: Done  24 out of  24 | elapsed:  8.5min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2344/2344 [==============================] - 14s 6ms/step - loss: 0.7348 - accuracy: 0.7743\n",
            "Epoch 2/3\n",
            "2344/2344 [==============================] - 13s 6ms/step - loss: 0.4181 - accuracy: 0.8712\n",
            "Epoch 3/3\n",
            "2344/2344 [==============================] - 14s 6ms/step - loss: 0.3222 - accuracy: 0.9007\n",
            "Best: 0.8656399846076965 using {'epochs': 3, 'first_layer_nodes': 500, 'last_layer_nodes': 50, 'n_layers': 3}\n",
            "Means: 0.864840010801951, Stdev: 0.00272725015572332 with: {'epochs': 3, 'first_layer_nodes': 500, 'last_layer_nodes': 100, 'n_layers': 2}\n",
            "Means: 0.8649999896685282, Stdev: 0.0018122450013637408 with: {'epochs': 3, 'first_layer_nodes': 500, 'last_layer_nodes': 100, 'n_layers': 3}\n",
            "Means: 0.8655466834704081, Stdev: 0.001685174074123665 with: {'epochs': 3, 'first_layer_nodes': 500, 'last_layer_nodes': 50, 'n_layers': 2}\n",
            "Means: 0.8656399846076965, Stdev: 0.0028108449074578795 with: {'epochs': 3, 'first_layer_nodes': 500, 'last_layer_nodes': 50, 'n_layers': 3}\n",
            "Means: 0.8621066610018412, Stdev: 0.0007792959923252384 with: {'epochs': 3, 'first_layer_nodes': 300, 'last_layer_nodes': 100, 'n_layers': 2}\n",
            "Means: 0.8607866565386454, Stdev: 0.0018342980822800444 with: {'epochs': 3, 'first_layer_nodes': 300, 'last_layer_nodes': 100, 'n_layers': 3}\n",
            "Means: 0.8613066871960958, Stdev: 0.0007349076581123277 with: {'epochs': 3, 'first_layer_nodes': 300, 'last_layer_nodes': 50, 'n_layers': 2}\n",
            "Means: 0.8613600134849548, Stdev: 0.0022359572792476795 with: {'epochs': 3, 'first_layer_nodes': 300, 'last_layer_nodes': 50, 'n_layers': 3}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJ2gBNkSh_oB"
      },
      "source": [
        "best_model = grid_result.best_estimator_"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e27Lkm5rh_oB",
        "outputId": "222ecdfc-2866-4ae8-caeb-a0ac6a8b7abd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "best_model.get_params()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'build_fn': <function __main__.create_model>,\n",
              " 'epochs': 3,\n",
              " 'first_layer_nodes': 500,\n",
              " 'last_layer_nodes': 50,\n",
              " 'n_layers': 3}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMF5jY0Nh_oB"
      },
      "source": [
        "-----\n",
        "\n",
        "# Experiment 2\n",
        "\n",
        "## Benchmark different Optimization Algorithms \n",
        "\n",
        "In this section, we are going to use the same model and dataset in order to benchmark 3 different gridsearch approaches: \n",
        "\n",
        "- Random Search\n",
        "- Bayesian Optimization. \n",
        "- Brute Force Gridsearch\n",
        "\n",
        "Our goal in this experiment is two-fold. We want to see which appraoch \n",
        "\n",
        "- Scores the highest accuracy\n",
        "- Has the shortest run time \n",
        "\n",
        "We want to see how these 3 gridsearch approaches handle these trade-offs and to give you a sense of those trades offs.\n",
        "\n",
        "### Trade-offs\n",
        "\n",
        "`Brute Force Gridsearch` will train a model on every single unique hyperparameter combination, this guarantees that you'll get the highest possible accuracy from your parameter set but your gridsearch might have a very long run-time. \n",
        "\n",
        "`Random Search` will randomly sample from your parameter set which, depending on how many samples, the run-time might be significantly cut down but you might or might not sample the parameters that correspond to the heightest possible accuracies. \n",
        "\n",
        "`Bayesian Optimization` has a bit of intelligence built into it's search algorithm but you do need to manually select some parameters which greatly influence the model learning outcomes. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Kjv3A2Mh_oC"
      },
      "source": [
        "-------\n",
        "### Build our model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YK7wxH-_h_oC"
      },
      "source": [
        "# because gridsearching can take a lot of time and we are bench marking 3 different approaches\n",
        "# let's build a simple model to minimize run time \n",
        "\n",
        "def build_model(hp):\n",
        "    \n",
        "    \"\"\"\n",
        "    Returns a complied keras model ready for keras-tuner gridsearch algorithms \n",
        "    \"\"\"\n",
        "    \n",
        "    model = Sequential()\n",
        "    \n",
        "    # hidden layer\n",
        "    model.add(Dense(units=hp.get('units'),activation=hp.get(\"activation\")))\n",
        "    \n",
        "    # output layer\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "    \n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(hp.get('learning_rate')),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy'])\n",
        "    \n",
        "    return model\n",
        "  "
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0ynO74yh_oC",
        "outputId": "65e2d9d9-4157-41e3-95b4-e1633b661d53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# build out our hyperparameter dictionary \n",
        "hp = HyperParameters()\n",
        "hp.Int('units', min_value=32, max_value=512, step=32)\n",
        "hp.Choice('learning_rate',values=[1e-1, 1e-2, 1e-3])\n",
        "hp.Choice('activation',values=[\"relu\", \"sigmoid\"])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'relu'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MFsLYAgh_oC"
      },
      "source": [
        "------\n",
        "# Run the Gridsearch Algorithms \n",
        "\n",
        "### Random Search\n",
        "\n",
        "Be sure to check out the [**docs for Keras-Tuner**](https://keras-team.github.io/keras-tuner/documentation/tuners/). Here you can read about the input parameters for the `RandomSearch` tuner."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "aaff9aae33845f374e15f2381719d83a",
          "grade": false,
          "grade_id": "cell-8c1dfb9b6d12bea2",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "AQ37stWxh_oC",
        "outputId": "29e02fad-be48-4525-aca0-5343c71cf934",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# how many unique hyperparameter combinations do we have? \n",
        "# HINT: take the product of the number of possible values for each hyperparameter \n",
        "# save your answer to n_unique_hparam_combos\n",
        "\n",
        "# YOUR CODE HERE\n",
        "n_unique_hparam_combos = ((512 - 32) / 32) * 3 * 2\n",
        "n_unique_hparam_combos"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "90.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "a9d628451e83431e1b52da10eccf2c00",
          "grade": false,
          "grade_id": "cell-1fa83950bb2d5f92",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "-L2Z4VqKh_oC",
        "outputId": "57e1c650-1b30-4315-afce-0838c17a9276",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# how many of these do we want to randomly sample?\n",
        "# let's pick 25% of n_unique_hparam_combos param combos to sample\n",
        "# save this number to n_param_combos_to_sample\n",
        "\n",
        "# YOUR CODE HERE\n",
        "n_param_combos_to_sample = int(0.25 * n_unique_hparam_combos)\n",
        "n_param_combos_to_sample"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8fuywbOh_oD"
      },
      "source": [
        "random_tuner = RandomSearch(\n",
        "            build_model,\n",
        "            objective='val_accuracy',\n",
        "            max_trials=n_param_combos_to_sample, # number of times to sample the parameter set and build a model \n",
        "            seed=1234,\n",
        "            hyperparameters=hp, # pass in our hyperparameter dictionary\n",
        "            directory='./keras-tuner-trial',\n",
        "            project_name='random_search')"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfAXO61Bh_oD",
        "outputId": "5261d6a0-1f91-425d-90cb-5b88c275789a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# take note of Total elapsed time in print out\n",
        "random_tuner.search(X_train, y_train,\n",
        "                    epochs=3,\n",
        "                    validation_data=(X_test, y_test))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trial 22 Complete [00h 00m 20s]\n",
            "val_accuracy: 0.18007999658584595\n",
            "\n",
            "Best val_accuracy So Far: 0.8726000189781189\n",
            "Total elapsed time: 00h 07m 56s\n",
            "INFO:tensorflow:Oracle triggered exit\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flJTHxI4h_oD",
        "outputId": "6ed89148-2c75-4661-98e0-750b7bceebb9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# identify the best score and hyperparamter (should be at the top since scores are ranked)\n",
        "random_tuner.results_summary()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results summary\n",
            "Results in ./keras-tuner-trial/random_search\n",
            "Showing 10 best trials\n",
            "Objective(name='val_accuracy', direction='max')\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 352\n",
            "learning_rate: 0.001\n",
            "activation: relu\n",
            "Score: 0.8726000189781189\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 384\n",
            "learning_rate: 0.001\n",
            "activation: relu\n",
            "Score: 0.8697199821472168\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 480\n",
            "learning_rate: 0.001\n",
            "activation: sigmoid\n",
            "Score: 0.8674799799919128\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 416\n",
            "learning_rate: 0.001\n",
            "activation: sigmoid\n",
            "Score: 0.8650000095367432\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 448\n",
            "learning_rate: 0.001\n",
            "activation: sigmoid\n",
            "Score: 0.8602399826049805\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 224\n",
            "learning_rate: 0.001\n",
            "activation: sigmoid\n",
            "Score: 0.8552799820899963\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 320\n",
            "learning_rate: 0.01\n",
            "activation: sigmoid\n",
            "Score: 0.8431199789047241\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 192\n",
            "learning_rate: 0.01\n",
            "activation: sigmoid\n",
            "Score: 0.8373200297355652\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 160\n",
            "learning_rate: 0.01\n",
            "activation: sigmoid\n",
            "Score: 0.8369600176811218\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 192\n",
            "learning_rate: 0.01\n",
            "activation: relu\n",
            "Score: 0.8248800039291382\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "id": "Rywog71-h_oD"
      },
      "source": [
        " ### Results\n",
        " \n",
        "Identify and write the the best performing hyperparamter combination and model score. \n",
        "Note that because this is Random Search, multiple runs might have slighly different outcomes. \n",
        " \n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "f084b5d373f8589a1de8d6d4473b974a",
          "grade": true,
          "grade_id": "cell-5527738b6382c164",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "0xe4ktvRh_oD"
      },
      "source": [
        "Best performing hyperparamter combination:\n",
        "\n",
        "units: 352\n",
        "\n",
        "learning_rate: 0.001\n",
        "\n",
        "activation: relu\n",
        "\n",
        "Score: 0.8726000189781189"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJ_rw5bBh_oD"
      },
      "source": [
        "------\n",
        "### Bayesian Optimization\n",
        "\n",
        "![](https://upload.wikimedia.org/wikipedia/commons/0/02/GpParBayesAnimationSmall.gif)\n",
        "\n",
        "Be sure to check out the [**docs for Keras-Tuner**](https://keras-team.github.io/keras-tuner/documentation/tuners/). Here you can read about the input parameters for the `BayesianOptimization` tuner.\n",
        "\n",
        "Pay special attention to these `BayesianOptimization` parameters: `num_initial_points` and `beta`. \n",
        "\n",
        "`num_initial_points`: \n",
        "\n",
        "Number of randomly selected hyperparameter combinations to try before applying bayesian probability to determine liklihood of which param combo to try next based on expected improvement\n",
        "\n",
        "\n",
        "`beta`: \n",
        "\n",
        "Larger values means more willing to explore new hyperparameter combinations (analogous to searching for the global minimum in Gradient Descent), smaller values means that it is less willing to try new hyperparameter combinations (analogous to getting stuck in a local minimum in Gradient Descent). \n",
        "\n",
        "As a start, error on the side of larger values. What defines a small or large value you ask? That question would pull us into the mathematical intricacies of Bayesian Optimization and Gaussian Processes. For simplicity, notice that the default value is 2.6 and work from there. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKXQEBUQh_oD"
      },
      "source": [
        "# we know that 24 samples is about 25% of 96 possible hyper-parameter combos\n",
        "# because BO isn't random (after num_initial_points number of trails) let's see if 15 max trials gives good results\n",
        "# feel free to play with any of these numbers\n",
        "max_trials=15\n",
        "num_initial_points=5\n",
        "beta=5.0"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o19ml22oh_oE"
      },
      "source": [
        "bayesian_tuner = BayesianOptimization(\n",
        "                    build_model,\n",
        "                    objective='val_accuracy',\n",
        "                    max_trials=max_trials,\n",
        "                    hyperparameters=hp, # pass in our hyperparameter dictionary\n",
        "                    num_initial_points=num_initial_points, \n",
        "                    beta=beta, \n",
        "                    seed=1234,\n",
        "                    directory='./keras-tuner-trial',\n",
        "                    project_name='bayesian_optimization_4')"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loYxVww2h_oE",
        "outputId": "9a06678b-07f0-4687-a8b1-9b00af3f98e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "bayesian_tuner.search(X_train, y_train,\n",
        "               epochs=3,\n",
        "               validation_data=(X_test, y_test))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trial 15 Complete [00h 00m 12s]\n",
            "val_accuracy: 0.825160026550293\n",
            "\n",
            "Best val_accuracy So Far: 0.876479983329773\n",
            "Total elapsed time: 00h 05m 11s\n",
            "INFO:tensorflow:Oracle triggered exit\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "62SmQ9cGh_oE",
        "outputId": "9000207b-b3a3-4268-a075-fbb76b9708fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "bayesian_tuner.results_summary()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results summary\n",
            "Results in ./keras-tuner-trial/bayesian_optimization_4\n",
            "Showing 10 best trials\n",
            "Objective(name='val_accuracy', direction='max')\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 512\n",
            "learning_rate: 0.001\n",
            "activation: relu\n",
            "Score: 0.876479983329773\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 352\n",
            "learning_rate: 0.001\n",
            "activation: relu\n",
            "Score: 0.8718000054359436\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 480\n",
            "learning_rate: 0.001\n",
            "activation: sigmoid\n",
            "Score: 0.862280011177063\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 256\n",
            "learning_rate: 0.001\n",
            "activation: sigmoid\n",
            "Score: 0.8593599796295166\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 256\n",
            "learning_rate: 0.01\n",
            "activation: sigmoid\n",
            "Score: 0.8385999798774719\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 32\n",
            "learning_rate: 0.001\n",
            "activation: relu\n",
            "Score: 0.8285199999809265\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 32\n",
            "learning_rate: 0.001\n",
            "activation: relu\n",
            "Score: 0.8255199790000916\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 32\n",
            "learning_rate: 0.001\n",
            "activation: relu\n",
            "Score: 0.825160026550293\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 32\n",
            "learning_rate: 0.001\n",
            "activation: relu\n",
            "Score: 0.8241999745368958\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 384\n",
            "learning_rate: 0.01\n",
            "activation: relu\n",
            "Score: 0.8164399862289429\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UdwFZW-h_oE"
      },
      "source": [
        " ### Results\n",
        " \n",
        "Identify and write the the best performing hyperparamter combination and model score. \n",
        "Note that because this is  Bayesian Optimization, multiple runs might have slighly different outcomes. \n",
        " \n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "1badcdca408cdd49bc2e409dca3bac5a",
          "grade": true,
          "grade_id": "cell-ff95600bf745f40f",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "Ejv7moeah_oE"
      },
      "source": [
        "best performing hyperparamter combination:\n",
        "\n",
        "units: 512\n",
        "\n",
        "learning_rate: 0.001\n",
        "\n",
        "activation: relu\n",
        "\n",
        "Score: 0.876479983329773"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C62mgym5h_oE"
      },
      "source": [
        "---------\n",
        "## Brute Force Gridsearch Optimization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tHin0rih_oE"
      },
      "source": [
        "### Populate a Sklearn compatiable parameter dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kis-JI74h_oF"
      },
      "source": [
        "# build out our hyperparameter dictionary \n",
        "hyper_parameters = {\n",
        "    # BUG Fix: cast array as list otherwise GridSearchCV will throw error\n",
        "    \"units\": np.arange(32, 544, 32).tolist(),\n",
        "    \"learning_rate\": [1e-1, 1e-2, 1e-3],\n",
        "    \"activation\":[\"relu\", \"sigmoid\"]\n",
        "}"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IiMbP3q2h_oF",
        "outputId": "ad45484f-919a-466f-c867-2ef8c1ebca42",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "hyper_parameters"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'activation': ['relu', 'sigmoid'],\n",
              " 'learning_rate': [0.1, 0.01, 0.001],\n",
              " 'units': [32,\n",
              "  64,\n",
              "  96,\n",
              "  128,\n",
              "  160,\n",
              "  192,\n",
              "  224,\n",
              "  256,\n",
              "  288,\n",
              "  320,\n",
              "  352,\n",
              "  384,\n",
              "  416,\n",
              "  448,\n",
              "  480,\n",
              "  512]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkFXVay4h_oF"
      },
      "source": [
        "### Build a Sklearn compatiable model function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfJdTiEoh_oF"
      },
      "source": [
        "def build_model(units, learning_rate, activation):\n",
        "    \n",
        "    \"\"\"\n",
        "    Returns a complie keras model ready for keras-tuner gridsearch algorithms \n",
        "    \"\"\"\n",
        "    \n",
        "    model = Sequential()\n",
        "    \n",
        "    # hidden layer\n",
        "    model.add(Dense(units, activation=activation))\n",
        "    \n",
        "    # output layer\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "    \n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7ciWpzhh_oF"
      },
      "source": [
        "model = KerasClassifier(build_fn = build_model)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "KNUCtSmPh_oF",
        "outputId": "422cceb0-e30b-4656-a5d1-46090f92a886",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# save start time \n",
        "start = time()\n",
        "\n",
        "# Create Grid Search\n",
        "grid = GridSearchCV(estimator=model, \n",
        "                    param_grid=hyper_parameters, \n",
        "                    n_jobs=-2, \n",
        "                    verbose=1, \n",
        "                    cv=3)\n",
        "\n",
        "grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "# save end time \n",
        "end = time()\n",
        "\n",
        "# Report Results\n",
        "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
        "\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 96 candidates, totalling 288 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-2)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.9492 - accuracy: 0.3382\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.9712 - accuracy: 0.3038\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 2.0710 - accuracy: 0.2464\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 2.0005 - accuracy: 0.2248\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 2.4028 - accuracy: 0.1407\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 2.3150 - accuracy: 0.0983\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 2.1396 - accuracy: 0.2669\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 2.3245 - accuracy: 0.2588\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.9896 - accuracy: 0.3327\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 2.0115 - accuracy: 0.2551\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 2.4155 - accuracy: 0.2496\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 2.1552 - accuracy: 0.1625\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 2.1571 - accuracy: 0.3479\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.8665 - accuracy: 0.3364\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 2.3250 - accuracy: 0.3418\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.8227 - accuracy: 0.3457\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 2.0143 - accuracy: 0.3537\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 2.2187 - accuracy: 0.2149\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 2.0297 - accuracy: 0.3825\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.9397 - accuracy: 0.3347\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 2.0851 - accuracy: 0.3736\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.8418 - accuracy: 0.3482\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 2.2943 - accuracy: 0.3353\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.8877 - accuracy: 0.2919\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 2.1686 - accuracy: 0.3540\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.8049 - accuracy: 0.3196\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 2.2471 - accuracy: 0.3477\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 2.0801 - accuracy: 0.3167\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 2.4491 - accuracy: 0.2954\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.9717 - accuracy: 0.2754\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 2.6227 - accuracy: 0.3392\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.8938 - accuracy: 0.3117\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 2.5197 - accuracy: 0.3239\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.9494 - accuracy: 0.2846\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 3.0487 - accuracy: 0.2729\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 2.1565 - accuracy: 0.1640\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 2.5854 - accuracy: 0.3413\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.9304 - accuracy: 0.2723\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 2.6452 - accuracy: 0.2946\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 2.0308 - accuracy: 0.2164\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 2.3274 - accuracy: 0.3449\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 2.4561 - accuracy: 0.3066\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 2.7097 - accuracy: 0.3674\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 2.0285 - accuracy: 0.2669\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 2.9263 - accuracy: 0.2598\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 2.1368 - accuracy: 0.1772\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 2.6061 - accuracy: 0.3672\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.9005 - accuracy: 0.3083\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 2.5438 - accuracy: 0.3330\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 2.0982 - accuracy: 0.3038\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 2.5593 - accuracy: 0.3753\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 2.0290 - accuracy: 0.2934\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 3.1173 - accuracy: 0.3548\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 2.0702 - accuracy: 0.2058\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 3.0780 - accuracy: 0.2613\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.9094 - accuracy: 0.2710\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 3.1261 - accuracy: 0.3111\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.8805 - accuracy: 0.3032\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 2.8481 - accuracy: 0.3465\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.9682 - accuracy: 0.2707\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 3.4340 - accuracy: 0.3164\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.8196 - accuracy: 0.3017\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 3.3182 - accuracy: 0.3253\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.9824 - accuracy: 0.2485\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 3.6124 - accuracy: 0.3195\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.9919 - accuracy: 0.2696\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 2.8330 - accuracy: 0.3997\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.9156 - accuracy: 0.2753\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 3.1110 - accuracy: 0.3180\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 2.0144 - accuracy: 0.2562\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 2.9063 - accuracy: 0.3341\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.9519 - accuracy: 0.2620\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 3.2180 - accuracy: 0.3197\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 2.1073 - accuracy: 0.1987\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 3.4041 - accuracy: 0.3404\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.8878 - accuracy: 0.2989\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 2.7370 - accuracy: 0.4089\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.8102 - accuracy: 0.3435\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 3.1866 - accuracy: 0.2599\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 2.0772 - accuracy: 0.1932\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 3.5194 - accuracy: 0.3537\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.8748 - accuracy: 0.3088\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 3.3887 - accuracy: 0.3647\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 2.1064 - accuracy: 0.2658\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 3.5293 - accuracy: 0.3301\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.8467 - accuracy: 0.3060\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 3.5183 - accuracy: 0.3437\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.9688 - accuracy: 0.2542\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 3.4018 - accuracy: 0.3486\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.8414 - accuracy: 0.3267\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 3.4329 - accuracy: 0.3876\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.9123 - accuracy: 0.2848\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 3.7078 - accuracy: 0.3548\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.9223 - accuracy: 0.3228\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 3.3378 - accuracy: 0.3597\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.9576 - accuracy: 0.2737\n",
            "1563/1563 [==============================] - 3s 1ms/step - loss: 0.9163 - accuracy: 0.7126\n",
            "782/782 [==============================] - 1s 995us/step - loss: 0.7192 - accuracy: 0.7836\n",
            "1563/1563 [==============================] - 2s 1ms/step - loss: 0.9121 - accuracy: 0.7153\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6820 - accuracy: 0.7947\n",
            "1563/1563 [==============================] - 3s 1ms/step - loss: 0.9177 - accuracy: 0.7168\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.7487 - accuracy: 0.7759\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8672 - accuracy: 0.7265\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6624 - accuracy: 0.8035\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8812 - accuracy: 0.7258\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6500 - accuracy: 0.8086\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8722 - accuracy: 0.7328\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6948 - accuracy: 0.7927\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8682 - accuracy: 0.7315\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6606 - accuracy: 0.8026\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8736 - accuracy: 0.7295\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6457 - accuracy: 0.8099\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8458 - accuracy: 0.7369\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6808 - accuracy: 0.7926\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8653 - accuracy: 0.7306\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6629 - accuracy: 0.8022\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8614 - accuracy: 0.7340\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6329 - accuracy: 0.8133\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8680 - accuracy: 0.7298\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6650 - accuracy: 0.8018\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8594 - accuracy: 0.7320\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.7166 - accuracy: 0.7974\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8571 - accuracy: 0.7372\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6605 - accuracy: 0.8038\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8708 - accuracy: 0.7335\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.7060 - accuracy: 0.7812\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8728 - accuracy: 0.7318\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6633 - accuracy: 0.7984\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8626 - accuracy: 0.7366\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6596 - accuracy: 0.8059\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8504 - accuracy: 0.7388\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.7009 - accuracy: 0.7923\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8712 - accuracy: 0.7329\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.7003 - accuracy: 0.8017\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8617 - accuracy: 0.7353\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6813 - accuracy: 0.7966\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8445 - accuracy: 0.7405\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6519 - accuracy: 0.8034\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8624 - accuracy: 0.7353\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6411 - accuracy: 0.8107\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8713 - accuracy: 0.7373\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6655 - accuracy: 0.8066\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.8686 - accuracy: 0.7370\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6496 - accuracy: 0.8086\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8629 - accuracy: 0.7390\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6559 - accuracy: 0.8045\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.8621 - accuracy: 0.7363\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6802 - accuracy: 0.8006\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.8500 - accuracy: 0.7413\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6676 - accuracy: 0.7986\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8590 - accuracy: 0.7411\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6424 - accuracy: 0.8120\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8650 - accuracy: 0.7354\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6423 - accuracy: 0.8116\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8544 - accuracy: 0.7398\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6874 - accuracy: 0.7866\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8913 - accuracy: 0.7283\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6942 - accuracy: 0.7894\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8840 - accuracy: 0.7344\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6434 - accuracy: 0.8099\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8603 - accuracy: 0.7395\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.7356 - accuracy: 0.7849\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.8585 - accuracy: 0.7392\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.7094 - accuracy: 0.7956\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8779 - accuracy: 0.7325\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6621 - accuracy: 0.8096\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8733 - accuracy: 0.7349\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6964 - accuracy: 0.8031\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.8685 - accuracy: 0.7412\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6372 - accuracy: 0.8140\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.8894 - accuracy: 0.7308\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6679 - accuracy: 0.8036\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.8547 - accuracy: 0.7433\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6532 - accuracy: 0.8092\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.8994 - accuracy: 0.7303\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6455 - accuracy: 0.8131\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.8776 - accuracy: 0.7353\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6616 - accuracy: 0.8067\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.8927 - accuracy: 0.7362\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.7035 - accuracy: 0.7918\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.8660 - accuracy: 0.7362\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.7069 - accuracy: 0.7890\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.8865 - accuracy: 0.7344\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6720 - accuracy: 0.8039\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.8690 - accuracy: 0.7407\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6600 - accuracy: 0.8000\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.8708 - accuracy: 0.7319\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6677 - accuracy: 0.8007\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.8880 - accuracy: 0.7338\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6466 - accuracy: 0.8075\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.8720 - accuracy: 0.7337\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6648 - accuracy: 0.8014\n",
            "1563/1563 [==============================] - 3s 1ms/step - loss: 1.0924 - accuracy: 0.6606\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.7250 - accuracy: 0.7888\n",
            "1563/1563 [==============================] - 3s 1ms/step - loss: 1.0892 - accuracy: 0.6665\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.7167 - accuracy: 0.7931\n",
            "1563/1563 [==============================] - 3s 1ms/step - loss: 1.0729 - accuracy: 0.6718\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.7341 - accuracy: 0.7842\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.9854 - accuracy: 0.6955\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6648 - accuracy: 0.8036\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.0049 - accuracy: 0.6918\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6462 - accuracy: 0.8139\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.9921 - accuracy: 0.7003\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6726 - accuracy: 0.8022\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.9494 - accuracy: 0.7116\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6077 - accuracy: 0.8179\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.9648 - accuracy: 0.7031\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6181 - accuracy: 0.8176\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.9512 - accuracy: 0.7081\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6361 - accuracy: 0.8141\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.9312 - accuracy: 0.7168\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.5920 - accuracy: 0.8270\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.9321 - accuracy: 0.7159\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.5945 - accuracy: 0.8246\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.9244 - accuracy: 0.7172\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.5982 - accuracy: 0.8204\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.9158 - accuracy: 0.7203\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.5691 - accuracy: 0.8340\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.9015 - accuracy: 0.7221\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.5715 - accuracy: 0.8316\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8985 - accuracy: 0.7267\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.5704 - accuracy: 0.8276\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8912 - accuracy: 0.7291\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.5566 - accuracy: 0.8365\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8880 - accuracy: 0.7280\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.5664 - accuracy: 0.8340\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8924 - accuracy: 0.7307\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.5812 - accuracy: 0.8295\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.8737 - accuracy: 0.7346\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.5537 - accuracy: 0.8386\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8880 - accuracy: 0.7298\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.5583 - accuracy: 0.8358\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8779 - accuracy: 0.7320\n",
            "782/782 [==============================] - 2s 1ms/step - loss: 0.5536 - accuracy: 0.8357\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.8742 - accuracy: 0.7337\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.5447 - accuracy: 0.8409\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.8788 - accuracy: 0.7304\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.5555 - accuracy: 0.8328\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.8672 - accuracy: 0.7341\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5637 - accuracy: 0.8289\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.8672 - accuracy: 0.7332\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5578 - accuracy: 0.8368\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8629 - accuracy: 0.7349\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5478 - accuracy: 0.8376\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8446 - accuracy: 0.7438\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5480 - accuracy: 0.8346\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8636 - accuracy: 0.7367\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5399 - accuracy: 0.8415\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8641 - accuracy: 0.7381\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.5383 - accuracy: 0.8428\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8577 - accuracy: 0.7379\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.5547 - accuracy: 0.8356\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8378 - accuracy: 0.7464\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5593 - accuracy: 0.8296\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8532 - accuracy: 0.7402\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5339 - accuracy: 0.8446\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8507 - accuracy: 0.7440\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5474 - accuracy: 0.8360\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.8413 - accuracy: 0.7430\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5370 - accuracy: 0.8442\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.8494 - accuracy: 0.7392\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5352 - accuracy: 0.8416\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.8391 - accuracy: 0.7445\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5466 - accuracy: 0.8334\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.8483 - accuracy: 0.7418\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5285 - accuracy: 0.8471\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.8414 - accuracy: 0.7431\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5377 - accuracy: 0.8406\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.8416 - accuracy: 0.7404\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5381 - accuracy: 0.8395\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.8348 - accuracy: 0.7443\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5371 - accuracy: 0.8423\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.8365 - accuracy: 0.7440\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5289 - accuracy: 0.8465\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.8355 - accuracy: 0.7475\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5412 - accuracy: 0.8362\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.8309 - accuracy: 0.7423\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5282 - accuracy: 0.8449\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.8346 - accuracy: 0.7465\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5357 - accuracy: 0.8411\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.8243 - accuracy: 0.7516\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5496 - accuracy: 0.8308\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.8361 - accuracy: 0.7390\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5156 - accuracy: 0.8470\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.8337 - accuracy: 0.7429\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5248 - accuracy: 0.8457\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.8349 - accuracy: 0.7428\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5649 - accuracy: 0.8298\n",
            "1563/1563 [==============================] - 3s 1ms/step - loss: 1.4016 - accuracy: 0.5319\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.1239 - accuracy: 0.6216\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.2402 - accuracy: 0.5916\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.9924 - accuracy: 0.6839\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.1727 - accuracy: 0.6151\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.0327 - accuracy: 0.6578\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.2597 - accuracy: 0.5964\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.0078 - accuracy: 0.6759\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.3266 - accuracy: 0.5700\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.0630 - accuracy: 0.6768\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.3871 - accuracy: 0.5588\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.1783 - accuracy: 0.6477\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.3045 - accuracy: 0.5863\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.0344 - accuracy: 0.6776\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.2646 - accuracy: 0.5947\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.0487 - accuracy: 0.6919\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.3776 - accuracy: 0.5639\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.1614 - accuracy: 0.6431\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.3912 - accuracy: 0.5844\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.3231 - accuracy: 0.5961\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.4239 - accuracy: 0.5632\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.1340 - accuracy: 0.6508\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.3977 - accuracy: 0.5786\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.1760 - accuracy: 0.6506\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.4323 - accuracy: 0.5793\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.1770 - accuracy: 0.6489\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.5170 - accuracy: 0.5493\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.3588 - accuracy: 0.6236\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.5996 - accuracy: 0.5440\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.2353 - accuracy: 0.6696\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.5210 - accuracy: 0.5750\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.0594 - accuracy: 0.6932\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.4580 - accuracy: 0.5712\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.3621 - accuracy: 0.6253\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.4237 - accuracy: 0.5905\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.2204 - accuracy: 0.6448\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 1.4449 - accuracy: 0.5853\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.1389 - accuracy: 0.6653\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 1.4825 - accuracy: 0.5782\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.2003 - accuracy: 0.6340\n",
            "1563/1563 [==============================] - 5s 2ms/step - loss: 1.4286 - accuracy: 0.5740\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.1075 - accuracy: 0.6535\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 1.5420 - accuracy: 0.5701\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.2373 - accuracy: 0.6166\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 2.0294 - accuracy: 0.5024\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.7080 - accuracy: 0.5359\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 1.8781 - accuracy: 0.5283\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.6806 - accuracy: 0.5043\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.5387 - accuracy: 0.5734\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.6070 - accuracy: 0.6243\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 1.5162 - accuracy: 0.5763\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.0963 - accuracy: 0.6836\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.8460 - accuracy: 0.5368\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.4707 - accuracy: 0.6318\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.5740 - accuracy: 0.5690\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.2972 - accuracy: 0.6336\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.9066 - accuracy: 0.5547\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.2210 - accuracy: 0.6894\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.5857 - accuracy: 0.5783\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.2877 - accuracy: 0.6427\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.3969 - accuracy: 0.5961\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.0895 - accuracy: 0.6637\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.5620 - accuracy: 0.5866\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.0675 - accuracy: 0.6924\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 2.3679 - accuracy: 0.5210\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.2855 - accuracy: 0.6646\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 2.1111 - accuracy: 0.5380\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.3468 - accuracy: 0.6274\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 2.0888 - accuracy: 0.5235\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.5749 - accuracy: 0.6337\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 2.4760 - accuracy: 0.5126\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.4129 - accuracy: 0.6530\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.7809 - accuracy: 0.5682\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.4262 - accuracy: 0.6307\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 2.5395 - accuracy: 0.5107\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.6664 - accuracy: 0.6083\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 2.0574 - accuracy: 0.5231\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.2026 - accuracy: 0.6437\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.8579 - accuracy: 0.5599\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.4832 - accuracy: 0.6132\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.9891 - accuracy: 0.5577\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.4870 - accuracy: 0.6523\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.6948 - accuracy: 0.5709\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.4266 - accuracy: 0.6381\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 2.1505 - accuracy: 0.5457\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.2900 - accuracy: 0.6430\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 2.3111 - accuracy: 0.5347\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.3500 - accuracy: 0.6654\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.9831 - accuracy: 0.5558\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.5667 - accuracy: 0.6102\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 2.4097 - accuracy: 0.5255\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.4474 - accuracy: 0.6279\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 2.0591 - accuracy: 0.5504\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.1357 - accuracy: 0.6750\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 2.0783 - accuracy: 0.5344\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.5175 - accuracy: 0.6340\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.9242 - accuracy: 0.7163\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6815 - accuracy: 0.7956\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.9306 - accuracy: 0.7130\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6945 - accuracy: 0.7908\n",
            "1563/1563 [==============================] - 3s 1ms/step - loss: 0.9299 - accuracy: 0.7129\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.7018 - accuracy: 0.7863\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8675 - accuracy: 0.7275\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6534 - accuracy: 0.8008\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8902 - accuracy: 0.7233\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6134 - accuracy: 0.8169\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8584 - accuracy: 0.7324\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6428 - accuracy: 0.8038\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8489 - accuracy: 0.7327\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6075 - accuracy: 0.8165\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8461 - accuracy: 0.7316\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6302 - accuracy: 0.8052\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8481 - accuracy: 0.7336\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6484 - accuracy: 0.8024\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8337 - accuracy: 0.7351\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6082 - accuracy: 0.8161\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8356 - accuracy: 0.7384\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6219 - accuracy: 0.8092\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8284 - accuracy: 0.7425\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6347 - accuracy: 0.8020\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8346 - accuracy: 0.7381\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6251 - accuracy: 0.8055\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8430 - accuracy: 0.7330\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.5881 - accuracy: 0.8249\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8255 - accuracy: 0.7416\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6138 - accuracy: 0.8089\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8168 - accuracy: 0.7430\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5981 - accuracy: 0.8206\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8306 - accuracy: 0.7384\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6146 - accuracy: 0.8130\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8277 - accuracy: 0.7394\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6114 - accuracy: 0.8141\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.8261 - accuracy: 0.7446\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.5906 - accuracy: 0.8285\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8301 - accuracy: 0.7423\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.5861 - accuracy: 0.8232\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.8177 - accuracy: 0.7503\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6234 - accuracy: 0.8117\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.8281 - accuracy: 0.7378\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5908 - accuracy: 0.8256\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.8451 - accuracy: 0.7345\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6555 - accuracy: 0.8016\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.8254 - accuracy: 0.7418\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6289 - accuracy: 0.8011\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8109 - accuracy: 0.7460\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5876 - accuracy: 0.8255\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8300 - accuracy: 0.7426\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6103 - accuracy: 0.8138\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8255 - accuracy: 0.7426\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5928 - accuracy: 0.8198\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8223 - accuracy: 0.7434\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5951 - accuracy: 0.8220\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8379 - accuracy: 0.7389\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5772 - accuracy: 0.8244\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8230 - accuracy: 0.7441\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6010 - accuracy: 0.8167\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8386 - accuracy: 0.7439\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6248 - accuracy: 0.8122\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8424 - accuracy: 0.7385\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5896 - accuracy: 0.8215\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8332 - accuracy: 0.7407\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5918 - accuracy: 0.8184\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.8435 - accuracy: 0.7360\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6102 - accuracy: 0.8172\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.8245 - accuracy: 0.7455\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6095 - accuracy: 0.8133\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.8380 - accuracy: 0.7424\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6355 - accuracy: 0.8020\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.8502 - accuracy: 0.7377\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6177 - accuracy: 0.8164\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.8348 - accuracy: 0.7397\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6568 - accuracy: 0.7953\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.8464 - accuracy: 0.7406\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6286 - accuracy: 0.8081\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.8388 - accuracy: 0.7429\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6635 - accuracy: 0.8014\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.8439 - accuracy: 0.7367\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5866 - accuracy: 0.8204\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.8835 - accuracy: 0.7349\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6016 - accuracy: 0.8187\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.8374 - accuracy: 0.7433\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5784 - accuracy: 0.8252\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.8504 - accuracy: 0.7417\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5808 - accuracy: 0.8222\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.8370 - accuracy: 0.7435\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6095 - accuracy: 0.8137\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.8679 - accuracy: 0.7372\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5891 - accuracy: 0.8238\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.8649 - accuracy: 0.7353\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5780 - accuracy: 0.8258\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.8451 - accuracy: 0.7396\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6062 - accuracy: 0.8099\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.3220 - accuracy: 0.6281\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.8171 - accuracy: 0.7604\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.3204 - accuracy: 0.6200\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.8008 - accuracy: 0.7679\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.2978 - accuracy: 0.6302\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.8192 - accuracy: 0.7568\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.1809 - accuracy: 0.6615\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.7487 - accuracy: 0.7794\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.1896 - accuracy: 0.6618\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.7437 - accuracy: 0.7830\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.1953 - accuracy: 0.6564\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.7654 - accuracy: 0.7707\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.1346 - accuracy: 0.6597\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.7351 - accuracy: 0.7850\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.1423 - accuracy: 0.6621\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.7237 - accuracy: 0.7888\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.1247 - accuracy: 0.6710\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.7213 - accuracy: 0.7869\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.1125 - accuracy: 0.6765\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.7105 - accuracy: 0.7903\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.0783 - accuracy: 0.6840\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6995 - accuracy: 0.7992\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.0796 - accuracy: 0.6832\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.7225 - accuracy: 0.7868\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.0537 - accuracy: 0.6845\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.7076 - accuracy: 0.7878\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.0648 - accuracy: 0.6844\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6951 - accuracy: 0.7989\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.0609 - accuracy: 0.6888\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.7051 - accuracy: 0.7905\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 1.0444 - accuracy: 0.6893\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.7006 - accuracy: 0.7946\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 1.0288 - accuracy: 0.6942\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6828 - accuracy: 0.8021\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 1.0485 - accuracy: 0.6897\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6958 - accuracy: 0.7929\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 1.0340 - accuracy: 0.6891\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6801 - accuracy: 0.8004\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 1.0193 - accuracy: 0.6984\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6781 - accuracy: 0.8034\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 1.0333 - accuracy: 0.6920\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6954 - accuracy: 0.7956\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.0348 - accuracy: 0.6919\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6776 - accuracy: 0.7990\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.0246 - accuracy: 0.6926\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6764 - accuracy: 0.8003\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.0221 - accuracy: 0.6954\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6915 - accuracy: 0.7941\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.0133 - accuracy: 0.6936\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6762 - accuracy: 0.8018\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.0205 - accuracy: 0.6902\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6765 - accuracy: 0.8024\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.0064 - accuracy: 0.6999\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6861 - accuracy: 0.7933\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.0004 - accuracy: 0.6960\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6880 - accuracy: 0.7974\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.0079 - accuracy: 0.6986\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6673 - accuracy: 0.8051\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.0015 - accuracy: 0.7003\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6870 - accuracy: 0.7924\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 1.0033 - accuracy: 0.6992\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6852 - accuracy: 0.7978\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.0180 - accuracy: 0.6902\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6671 - accuracy: 0.8052\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.9859 - accuracy: 0.7011\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6828 - accuracy: 0.7945\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.9859 - accuracy: 0.7023\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6731 - accuracy: 0.8003\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.9985 - accuracy: 0.6971\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6633 - accuracy: 0.8060\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.9896 - accuracy: 0.7029\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6785 - accuracy: 0.7981\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.9883 - accuracy: 0.6999\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6568 - accuracy: 0.8076\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.9860 - accuracy: 0.7043\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6648 - accuracy: 0.8054\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.9786 - accuracy: 0.7068\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6751 - accuracy: 0.7993\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.9742 - accuracy: 0.7044\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6678 - accuracy: 0.8066\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.9869 - accuracy: 0.6990\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6622 - accuracy: 0.8060\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.9934 - accuracy: 0.6993\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6676 - accuracy: 0.8010\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.9899 - accuracy: 0.6971\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6774 - accuracy: 0.7988\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.9810 - accuracy: 0.7039\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6574 - accuracy: 0.8077\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.9855 - accuracy: 0.7051\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6718 - accuracy: 0.7991\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.9781 - accuracy: 0.7019\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6671 - accuracy: 0.8063\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.9721 - accuracy: 0.7048\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6528 - accuracy: 0.8095\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.9761 - accuracy: 0.7034\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6865 - accuracy: 0.7908\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-2)]: Done 288 out of 288 | elapsed: 30.6min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2344/2344 [==============================] - 8s 3ms/step - loss: 0.7734 - accuracy: 0.7667\n",
            "Best: 0.8424266576766968 using {'activation': 'relu', 'learning_rate': 0.001, 'units': 416}\n",
            "Means: 0.2089733307560285, Stdev: 0.08461761441906562 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 32}\n",
            "Means: 0.22546667357285818, Stdev: 0.0445348147219441 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 64}\n",
            "Means: 0.29899999996026355, Stdev: 0.05960338794323718 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 96}\n",
            "Means: 0.32495999336242676, Stdev: 0.024005946152740067 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 128}\n",
            "Means: 0.3039066791534424, Stdev: 0.020193446422544493 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 160}\n",
            "Means: 0.2534533341725667, Stdev: 0.06418124548476284 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 192}\n",
            "Means: 0.2650800049304962, Stdev: 0.03715810347557034 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 224}\n",
            "Means: 0.2508133302132289, Stdev: 0.0547058453378958 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 256}\n",
            "Means: 0.26766666769981384, Stdev: 0.04394875203185496 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 288}\n",
            "Means: 0.2816133399804433, Stdev: 0.01523635776374517 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 320}\n",
            "Means: 0.27325333654880524, Stdev: 0.021871898338513416 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 352}\n",
            "Means: 0.26450666785240173, Stdev: 0.007972571692968718 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 384}\n",
            "Means: 0.28038666148980457, Stdev: 0.06054955592805119 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 416}\n",
            "Means: 0.25589332977930707, Stdev: 0.047706413744470995 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 448}\n",
            "Means: 0.2956666648387909, Stdev: 0.03048542537731694 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 480}\n",
            "Means: 0.2937333285808563, Stdev: 0.02101748018172008 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 512}\n",
            "Means: 0.784719983736674, Stdev: 0.007735017804369413 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 32}\n",
            "Means: 0.8015733559926351, Stdev: 0.006621684670531429 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 64}\n",
            "Means: 0.801693320274353, Stdev: 0.0070973813534225124 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 96}\n",
            "Means: 0.8057466745376587, Stdev: 0.005357625752072092 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 128}\n",
            "Means: 0.794159988562266, Stdev: 0.00950660084725512 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 160}\n",
            "Means: 0.79885333776474, Stdev: 0.005561420135615682 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 192}\n",
            "Means: 0.8005599975585938, Stdev: 0.002918264486858368 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 224}\n",
            "Means: 0.8086400032043457, Stdev: 0.001698331903495482 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 256}\n",
            "Means: 0.8012133439381918, Stdev: 0.002444550517327942 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 288}\n",
            "Means: 0.8033733169237772, Stdev: 0.011861654752086675 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 320}\n",
            "Means: 0.7947333256403605, Stdev: 0.010895999308694379 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 352}\n",
            "Means: 0.8027333418528239, Stdev: 0.0057207301692463975 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 384}\n",
            "Means: 0.8089200059572855, Stdev: 0.0042818007021233236 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 416}\n",
            "Means: 0.8038533528645834, Stdev: 0.008936769463541478 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 448}\n",
            "Means: 0.7976133227348328, Stdev: 0.006320537216871474 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 480}\n",
            "Means: 0.8032133181889852, Stdev: 0.003061030277011249 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 512}\n",
            "Means: 0.7886666655540466, Stdev: 0.0036421541764204445 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 32}\n",
            "Means: 0.8065600196520487, Stdev: 0.0052111714040230205 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 64}\n",
            "Means: 0.8165333469708761, Stdev: 0.001739690887201197 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 96}\n",
            "Means: 0.8240000009536743, Stdev: 0.0027072345621564253 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 128}\n",
            "Means: 0.8310799996058146, Stdev: 0.002674912480065723 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 160}\n",
            "Means: 0.8333333333333334, Stdev: 0.0028802597521386726 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 192}\n",
            "Means: 0.8367066582043966, Stdev: 0.0013391788020181402 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 224}\n",
            "Means: 0.8342000047365824, Stdev: 0.004992457382936074 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 256}\n",
            "Means: 0.836359977722168, Stdev: 0.0012866548688729607 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 288}\n",
            "Means: 0.8399866819381714, Stdev: 0.003123304464287892 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 320}\n",
            "Means: 0.8367466727892557, Stdev: 0.006144095180580877 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 352}\n",
            "Means: 0.839733342329661, Stdev: 0.0045711405326364085 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 384}\n",
            "Means: 0.8424266576766968, Stdev: 0.0033500370617737243 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 416}\n",
            "Means: 0.8416799902915955, Stdev: 0.004218186780524397 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 448}\n",
            "Means: 0.8389466603597006, Stdev: 0.005965793999412354 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 480}\n",
            "Means: 0.8408400019009908, Stdev: 0.007794629234583426 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 512}\n",
            "Means: 0.6544400254885355, Stdev: 0.0255338516454189 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 32}\n",
            "Means: 0.6667866706848145, Stdev: 0.013486972528502731 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 64}\n",
            "Means: 0.6708799997965494, Stdev: 0.020481355109392974 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 96}\n",
            "Means: 0.6325066685676575, Stdev: 0.025757727008546512 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 128}\n",
            "Means: 0.6473733385403951, Stdev: 0.01877864124514708 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 160}\n",
            "Means: 0.6544133424758911, Stdev: 0.028533429580275493 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 192}\n",
            "Means: 0.6509199937184652, Stdev: 0.01292999451124609 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 224}\n",
            "Means: 0.5522799889246622, Stdev: 0.047307002832915654 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 256}\n",
            "Means: 0.646559993426005, Stdev: 0.026344380612826967 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 288}\n",
            "Means: 0.6552133361498514, Stdev: 0.024433221404687778 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 320}\n",
            "Means: 0.6735600034395853, Stdev: 0.01332716769203468 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 352}\n",
            "Means: 0.6380399862925211, Stdev: 0.010857095271967545 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 384}\n",
            "Means: 0.6275733311971029, Stdev: 0.014606123320783966 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 416}\n",
            "Means: 0.634546677271525, Stdev: 0.01614879450933375 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 448}\n",
            "Means: 0.6395466526349386, Stdev: 0.02263781459494131 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 480}\n",
            "Means: 0.6456266641616821, Stdev: 0.020893709814081673 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 512}\n",
            "Means: 0.7908666531244913, Stdev: 0.0037893055395449063 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 32}\n",
            "Means: 0.8071600000063578, Stdev: 0.007006344429032414 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 64}\n",
            "Means: 0.8080533146858215, Stdev: 0.006067207545502175 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 96}\n",
            "Means: 0.8091066678365072, Stdev: 0.005748279042555793 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 128}\n",
            "Means: 0.8130933245023092, Stdev: 0.008477130482364538 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 160}\n",
            "Means: 0.8158933520317078, Stdev: 0.003387406095073036 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 192}\n",
            "Means: 0.8211333354314169, Stdev: 0.0070066820579725575 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 224}\n",
            "Means: 0.8094133138656616, Stdev: 0.011418825642683829 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 256}\n",
            "Means: 0.8196933269500732, Stdev: 0.0047849030902566425 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 288}\n",
            "Means: 0.8210533459981283, Stdev: 0.0032219795705978236 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 320}\n",
            "Means: 0.8173733353614807, Stdev: 0.0038414266658108554 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 352}\n",
            "Means: 0.8108266592025757, Stdev: 0.006443304234849799 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 384}\n",
            "Means: 0.8066133260726929, Stdev: 0.008703985543712444 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 416}\n",
            "Means: 0.813479999701182, Stdev: 0.008598844977364545 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 448}\n",
            "Means: 0.8203466733296713, Stdev: 0.004843191207923059 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 480}\n",
            "Means: 0.8198266625404358, Stdev: 0.007084415945613838 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 512}\n",
            "Means: 0.761680006980896, Stdev: 0.0046186979657771784 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 32}\n",
            "Means: 0.7776933312416077, Stdev: 0.0051723627855774865 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 64}\n",
            "Means: 0.786893347899119, Stdev: 0.0015513619055881123 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 96}\n",
            "Means: 0.7920799851417542, Stdev: 0.005213022697525257 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 128}\n",
            "Means: 0.7924000024795532, Stdev: 0.0047146713838351955 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 160}\n",
            "Means: 0.7965599894523621, Stdev: 0.00399371590641224 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 192}\n",
            "Means: 0.7997999986012777, Stdev: 0.003192655733566622 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 224}\n",
            "Means: 0.7977733214696249, Stdev: 0.002666578995688542 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 256}\n",
            "Means: 0.7991866668065389, Stdev: 0.0041838164672589155 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 288}\n",
            "Means: 0.7982800006866455, Stdev: 0.005217301949540446 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 320}\n",
            "Means: 0.7991466720898946, Stdev: 0.004462848341140329 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 352}\n",
            "Means: 0.801466683546702, Stdev: 0.003301759610458975 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 384}\n",
            "Means: 0.8040933410326639, Stdev: 0.003520064876522568 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 416}\n",
            "Means: 0.8045466542243958, Stdev: 0.0024886540315708938 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 448}\n",
            "Means: 0.8018933335940043, Stdev: 0.00412166490625131 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 480}\n",
            "Means: 0.8022133509318033, Stdev: 0.008150202829163539 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 512}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gji0iOKfh_oF",
        "outputId": "40953524-3f6a-4700-d122-b943bbce22fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# total run time \n",
        "total_run_time_in_miniutes = (end - start)/60\n",
        "total_run_time_in_miniutes"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30.69886084794998"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jh2bcHDqh_oF",
        "outputId": "d2aaa18a-19ea-4664-a47e-9ec25765796a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "grid_result.best_params_"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'activation': 'relu', 'learning_rate': 0.001, 'units': 416}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_-USbCFh_oG",
        "outputId": "cf9ef24b-62be-4bdc-84a6-c31a2c8c7c22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# because all other optimization approaches are reporting test set score\n",
        "# let's calculate the test set score in this case \n",
        "best_model = grid_result.best_estimator_\n",
        "test_acc = best_model.score(X_test, y_test)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4992 - accuracy: 0.8540\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LLMB3nvh_oG",
        "outputId": "80faeee9-4e6a-4b7c-a735-afc27fa3131a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "test_acc"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8539999723434448"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tec2FJjBh_oG"
      },
      "source": [
        " ### Results\n",
        " \n",
        "Identify and write the the best performing hyperparamter combination and model score. \n",
        " \n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "9577db883482c6cded3836e5cfbf5a74",
          "grade": true,
          "grade_id": "cell-eb06d682d2790f6e",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "NP9iD3PZh_oG"
      },
      "source": [
        " best performing hyperparamter combination and model score:\n",
        "\n",
        " 'activation': 'relu', \n",
        " \n",
        " 'learning_rate': 0.001, \n",
        " \n",
        " 'units': 416\n",
        "\n",
        " accuracy: 0.8540"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YPgMtNCh_oG"
      },
      "source": [
        "_______\n",
        "\n",
        "# Conclusion\n",
        "\n",
        "The spirit of this experiment is to expose you to the idea of benchmarking and comparing the trade-offs of various gridsearch approaches. \n",
        "\n",
        "Even if we did find a way to pass in the original test set into GridSearchCV, we can see that both Random Search and Bayesian Optimization are arguably better alternatives to a brute force grid search when we consider the trade-offs of run time and locating the best performing model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvZOPfM1eU58"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15p5giYTh_oG"
      },
      "source": [
        "----\n",
        "\n",
        "# Stretch Goals\n",
        "\n",
        "- Feel free to run whatever gridserach experiments on whatever models you like!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4NvDPAfh_oH"
      },
      "source": [
        "# this is your open playground - be free to explore as you wish "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}